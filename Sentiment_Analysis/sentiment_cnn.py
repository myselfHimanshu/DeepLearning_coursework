# -*- coding: utf-8 -*-
"""Code1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M_Cbv2WStsNcgc_lsGTjUSHY_EMsep5A
"""

!apt-get install -y -qq software-properties-common python-software-properties module-init-tools
!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null
!apt-get update -qq 2>&1 > /dev/null
!apt-get -y install -qq google-drive-ocamlfuse fuse
from google.colab import auth
auth.authenticate_user()
from oauth2client.client import GoogleCredentials
creds = GoogleCredentials.get_application_default()
import getpass
!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL
vcode = getpass.getpass()
!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}

!mkdir -p drive
!google-drive-ocamlfuse drive

#!ls drive

import os
os.chdir("drive/COLAB_DEEPLEARNING/sentiment_analysis/")

!ls

import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np

csv = "clean_tweet.csv"
my_df = pd.read_csv(csv)
my_df.head()

my_df.dropna(inplace=True)
my_df.reset_index(drop=True, inplace=True)
my_df.info()

X = my_df.text
y = my_df.target

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import os, sys

BASE_DIR = ''
GLOVE_DIR = BASE_DIR
MAX_SEQUENCE_LENGTH = 100
MAX_NUM_WORDS = 100000
EMBEDDING_DIM = 300
VALIDATION_SPLIT = 0.2

embeddings_index = {}
with open(os.path.join(GLOVE_DIR, 'glove.6B.300d.txt')) as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs

tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)
tokenizer.fit_on_texts(X)
sequences = tokenizer.texts_to_sequences(X)

word_index = tokenizer.word_index

import json
with open('dictionary.json', 'w') as dictionary_file:
    json.dump(word_index, dictionary_file)

data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)

from keras.utils import to_categorical

labels = to_categorical(np.asarray(y))
print('Shape of data tensor:', data.shape)
print('Shape of label tensor:', labels.shape)

from keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout
from keras.layers import Conv1D, MaxPooling1D, Embedding
from keras.models import Model
from keras.layers import concatenate, Activation

indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]
num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])

x_train = data[:-num_validation_samples]
y_train = labels[:-num_validation_samples]
x_val = data[-num_validation_samples:]
y_val = labels[-num_validation_samples:]

num_words = min(MAX_NUM_WORDS, len(word_index) + 1)
embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))
for word, i in word_index.items():
    if i >= MAX_NUM_WORDS:
        continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector

embedding_layer = Embedding(num_words,
                            EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=MAX_SEQUENCE_LENGTH,
                            trainable=False)

sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')
embedded_sequences = embedding_layer(sequence_input)

x1 = Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1)(embedded_sequences)
x1 = GlobalMaxPooling1D()(x1)

x2 = Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu', strides=1)(embedded_sequences)
x2 = GlobalMaxPooling1D()(x2)

x3 = Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu', strides=1)(embedded_sequences)
x3 = GlobalMaxPooling1D()(x3)

merged = concatenate([x1, x2, x3], axis=1)
merged = Dense(256, activation='relu')(merged)
merged = Dropout(0.2)(merged)
merged = Dense(2)(merged)
output = Activation('sigmoid')(merged)

model = Model(inputs=[sequence_input], outputs=[output])
model.compile(loss='binary_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])
model.summary()

from keras.callbacks import ModelCheckpoint

filepath="CNN_best_weights.{epoch:02d}-{val_acc:.4f}.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')

model.fit(x_train, y_train,
          batch_size=64,
          epochs=5,
          validation_data=(x_val, y_val), callbacks=[checkpoint])

model_json = model.to_json()
with open('model.json', 'w') as json_file:
    json_file.write(model_json)

model.save_weights('model.h5')
