{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "with open('data/LabelledData.txt','r') as f:\n",
    "    for line in f:\n",
    "        text, label = map(str,line.split(\",,,\"))\n",
    "        texts.append(text.strip())\n",
    "        labels.append(label.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for text, label in zip(texts[:10], labels[:10]):\n",
    "#    print(text,\" -->\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the sentences\n",
    "import re\n",
    "def pre_process(text):\n",
    "    text = re.sub(r\"\\b's\\b\",\"is\",text)\n",
    "    text = re.sub(r\"[^a-z?\\.]\",\" \",text.lower())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts = [pre_process(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how did serfdom develop in and then leave russia ?  --> unknown\n",
      "what films featured the character popeye doyle ?  --> what\n",
      "how can i find a list of celebrities   real names ?  --> unknown\n",
      "what fowl grabs the spotlight after the chinese year of the monkey ?  --> what\n",
      "what is the full form of .com ?  --> what\n",
      "what contemptible scoundrel stole the cork from my lunch ?  --> what\n",
      "what team did baseball  s st. louis browns become ?  --> what\n",
      "what is the oldest profession ?  --> what\n",
      "what are liver enzymes ?  --> what\n",
      "name the scar faced bounty hunter of the old west .  --> unknown\n"
     ]
    }
   ],
   "source": [
    "for text, label in zip(processed_texts[:10], labels[:10]):\n",
    "    print(text,\" -->\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array(texts)\n",
    "y = np.array(labels, dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'affirmation': 104, 'unknown': 272, 'what': 609, 'when': 96, 'who': 402}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(y, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himanshu/anaconda3/envs/deeplearning/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import some keras libraries\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import os, sys\n",
    "\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model, Sequential\n",
    "from keras import utils\n",
    "from keras.layers import concatenate, Activation\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format our text samples and labels into tensors that can be fed into a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 45\n",
    "MAX_NUM_WORDS = 1000\n",
    "TESTING_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = X[indices]\n",
    "labels = y[indices]\n",
    "num_testing_samples = int(TESTING_SPLIT * X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = data[:-num_testing_samples]\n",
    "train_y = labels[:-num_testing_samples]\n",
    "test_x = data[-num_testing_samples:]\n",
    "test_y = labels[-num_testing_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binarize labels in a one-vs-all fashion\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_y)\n",
    "y_train = encoder.transform(train_y)\n",
    "y_test = encoder.transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "x_train = tokenizer.texts_to_matrix(train_x)\n",
    "x_test = tokenizer.texts_to_matrix(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (1335, 1000)\n",
      "x_test shape: (148, 1000)\n",
      "y_train shape: (1335, 5)\n",
      "y_test shape: (148, 5)\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3427 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need of embedding layer(mapping semantic meaning into a geometric space)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define the layers of our model we will use the Keras Sequential model. We will use the cross entropy loss function, since each of our question can only belong to one tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               512512    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 1285      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 645,125\n",
      "Trainable params: 645,125\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(MAX_NUM_WORDS,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the weights\n",
    "filepath=\"weights/best_weights.{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1201 samples, validate on 134 samples\n",
      "Epoch 1/5\n",
      "1201/1201 [==============================] - 1s 883us/step - loss: 1.0139 - acc: 0.6436 - val_loss: 0.4888 - val_acc: 0.8433\n",
      "Epoch 2/5\n",
      "1201/1201 [==============================] - 1s 695us/step - loss: 0.2716 - acc: 0.9334 - val_loss: 0.2193 - val_acc: 0.9478\n",
      "Epoch 3/5\n",
      "1201/1201 [==============================] - 1s 649us/step - loss: 0.1014 - acc: 0.9709 - val_loss: 0.1598 - val_acc: 0.9552\n",
      "Epoch 4/5\n",
      "1201/1201 [==============================] - 1s 742us/step - loss: 0.0514 - acc: 0.9875 - val_loss: 0.1604 - val_acc: 0.9552\n",
      "Epoch 5/5\n",
      "1201/1201 [==============================] - 1s 666us/step - loss: 0.0259 - acc: 0.9933 - val_loss: 0.1446 - val_acc: 0.9776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f260c6b2a58>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model on training set with validation split 0.1. Don't touch testing set\n",
    "model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 5 epochs:\n",
    "- loss: 0.075\n",
    "- accuracy: 0.9850\n",
    "- val_loss: 0.1217\n",
    "\n",
    "Evalaute on testing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148/148 [==============================] - 0s 171us/step\n",
      "Test score: 0.12457356752978789\n",
      "Test accuracy: 0.9662162162162162\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_labels = encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what does a nihilist believe in ? ...\n",
      "Actual label:unknown\n",
      "Predicted label: what\n",
      "\n",
      "name the ranger who was always after yogi bear . ? ...\n",
      "Actual label:unknown\n",
      "Predicted label: who\n",
      "\n",
      "what actor came to dinner in guess who 's coming to dinner ? ...\n",
      "Actual label:who\n",
      "Predicted label: what\n",
      "\n",
      "do i need to tear off the black outside before i put the new filter into the machine ? ...\n",
      "Actual label:affirmation\n",
      "Predicted label: unknown\n",
      "\n",
      "can i install on wall ? ...\n",
      "Actual label:affirmation\n",
      "Predicted label: unknown\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print the test cases where it fails\n",
    "for i in range(148):\n",
    "    prediction = model.predict(np.array([x_test[i]]))\n",
    "    predicted_label = text_labels[np.argmax(prediction)]\n",
    "    if test_y[i]!=predicted_label:\n",
    "        print(test_x[i][:100], \"...\")\n",
    "        print('Actual label:' + test_y[i])\n",
    "        print(\"Predicted label: \" + predicted_label + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
