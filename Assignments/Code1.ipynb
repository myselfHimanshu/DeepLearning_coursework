{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "with open('data/LabelledData.txt','r') as f:\n",
    "    for line in f:\n",
    "        text, label = map(str,line.split(\",,,\"))\n",
    "        texts.append(text.strip())\n",
    "        labels.append(label.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for text, label in zip(texts[:10], labels[:10]):\n",
    "#    print(text,\" -->\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def pre_process(text):\n",
    "    text = re.sub(r\"\\b's\\b\",\"is\",text)\n",
    "    text = re.sub(r\"[^a-z?\\.]\",\" \",text.lower())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts = [pre_process(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how did serfdom develop in and then leave russia ?  --> unknown\n",
      "what films featured the character popeye doyle ?  --> what\n",
      "how can i find a list of celebrities   real names ?  --> unknown\n",
      "what fowl grabs the spotlight after the chinese year of the monkey ?  --> what\n",
      "what is the full form of .com ?  --> what\n",
      "what contemptible scoundrel stole the cork from my lunch ?  --> what\n",
      "what team did baseball  s st. louis browns become ?  --> what\n",
      "what is the oldest profession ?  --> what\n",
      "what are liver enzymes ?  --> what\n",
      "name the scar faced bounty hunter of the old west .  --> unknown\n"
     ]
    }
   ],
   "source": [
    "for text, label in zip(processed_texts[:10], labels[:10]):\n",
    "    print(text,\" -->\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array(texts)\n",
    "y = np.array(labels, dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himanshu/anaconda3/envs/deeplearning/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 45\n",
    "MAX_NUM_WORDS = 50\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3675 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (1483, 45)\n",
      "Shape of label tensor: (1483, 5)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import concatenate, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1187, 45), (296, 45))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 45)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 45, 100)      367600      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 44, 50)       10050       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 43, 50)       15050       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 50)           0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 50)           0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 100)          0           global_max_pooling1d_5[0][0]     \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          25856       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 5)            1285        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 5)            0           dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 419,841\n",
      "Trainable params: 419,841\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = Embedding(vocab_size, 100)(sequence_input)\n",
    "\n",
    "x1 = Conv1D(filters=50, kernel_size=2, padding='valid', activation='relu', strides=1)(embedded_sequences)\n",
    "x1 = GlobalMaxPooling1D()(x1)\n",
    "\n",
    "x2 = Conv1D(filters=50, kernel_size=3, padding='valid', activation='relu', strides=1)(embedded_sequences)\n",
    "x2 = GlobalMaxPooling1D()(x2)\n",
    "\n",
    "merged = concatenate([x1, x2], axis=1)\n",
    "merged = Dense(256, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = Dense(5)(merged)\n",
    "output = Activation('sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[sequence_input], outputs=[output])\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath=\"weights/CNN_best_weights.{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1068 samples, validate on 119 samples\n",
      "Epoch 1/5\n",
      "1068/1068 [==============================] - 1s 1ms/step - loss: 0.5823 - acc: 0.8019 - val_loss: 0.4545 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.80000, saving model to weights/CNN_best_weights.01-0.8000.hdf5\n",
      "Epoch 2/5\n",
      "1068/1068 [==============================] - 1s 717us/step - loss: 0.4004 - acc: 0.8410 - val_loss: 0.3690 - val_acc: 0.8655\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.80000 to 0.86555, saving model to weights/CNN_best_weights.02-0.8655.hdf5\n",
      "Epoch 3/5\n",
      "1068/1068 [==============================] - 1s 726us/step - loss: 0.2776 - acc: 0.9086 - val_loss: 0.2442 - val_acc: 0.9176\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.86555 to 0.91765, saving model to weights/CNN_best_weights.03-0.9176.hdf5\n",
      "Epoch 4/5\n",
      "1068/1068 [==============================] - 1s 836us/step - loss: 0.1649 - acc: 0.9513 - val_loss: 0.1730 - val_acc: 0.9462\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.91765 to 0.94622, saving model to weights/CNN_best_weights.04-0.9462.hdf5\n",
      "Epoch 5/5\n",
      "1068/1068 [==============================] - 1s 899us/step - loss: 0.1119 - acc: 0.9590 - val_loss: 0.1421 - val_acc: 0.9546\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.94622 to 0.95462, saving model to weights/CNN_best_weights.05-0.9546.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6faca86588>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=5,\n",
    "          validation_split=0.1, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296/296 [==============================] - 0s 223us/step\n",
      "Test score: 0.07537008389025121\n",
      "Test accuracy: 0.9743243165918298\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_val, y_val,\n",
    "                       batch_size=64, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_labels = encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    prediction = model.predict(np.array([x_val[i]]))\n",
    "    predicted_label = text_labels[np.argmax(prediction)]\n",
    "    print(test_posts.iloc[i][:50], \"...\")\n",
    "    print('Actual label:' + test_tags.iloc[i])\n",
    "    print(\"Predicted label: \" + predicted_label + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
